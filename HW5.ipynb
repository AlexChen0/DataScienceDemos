{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov = datasets.fetch_covtype()\n",
    "cov = pd.DataFrame(data=np.c_[cov.data, cov.target], columns=cov.feature_names+['target'])\n",
    "values, counts = np.unique(cov['target'].values, return_counts=True)\n",
    "#for i in range(len(cov['target'])):\n",
    "#    if cov['target'][i] == values[counts.argmax()]:\n",
    "#        cov['target'][i] = 1\n",
    "#    else:\n",
    "#        cov['target'][i] = 0\n",
    "cov['target'] = cov['target'].apply(lambda x: 1 if x == values[counts.argmax()] else 0)\n",
    "np.unique(cov['target'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.73666162 0.52608569 0.63262289]\n",
      "[0.66552099 0.7611432  0.74502843]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "x = cov[cov.columns[:-1]]\n",
    "y = cov['target'].values\n",
    "logreg = LogisticRegression(solver='saga')\n",
    "GBC = GradientBoostingClassifier(random_state = 53)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(cross_val_score(logreg, x, y, cv=3, scoring='roc_auc'))\n",
    "print(cross_val_score(GBC, x, y, cv=3, scoring='roc_auc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.88580452 0.77300701 0.70304575]\n",
      "[0.66552349 0.7611432  0.74502581]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "Xscale = MinMaxScaler().fit(x).transform(x)\n",
    "print(cross_val_score(logreg, Xscale, y, cv=3, scoring='roc_auc'))\n",
    "print(cross_val_score(GBC, Xscale, y, cv=3, scoring='roc_auc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the results here suggest that logreg benefits from the minmax scaling. This is likely due to the fact that scaling within log regression will eliminate potential bias, but in GBC, a tree based model, it is largely of no use since it is not dependant on scaling\n"
     ]
    }
   ],
   "source": [
    "print(\"the results here suggest that logreg benefits from the minmax scaling. This is likely due to the fact that scaling within log regression will eliminate potential bias, but in GBC, a tree based model, it is largely of no use since it is not dependant on scaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36823827 0.51075911 0.42641674]\n",
      "[0.54301554 0.67007573 0.67260455]\n",
      "[0.7684583  0.68544368 0.68933439]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "z = StandardScaler().fit_transform(x)\n",
    "Xscale = GaussianRandomProjection(n_components=5).fit_transform(z)\n",
    "print(cross_val_score(logreg, Xscale, y, cv=3, scoring='roc_auc'))\n",
    "\n",
    "Xscale = GaussianRandomProjection(n_components=20).fit_transform(z)\n",
    "print(cross_val_score(logreg, Xscale, y, cv=3, scoring='roc_auc'))\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "Xscale = PCA(n_components=5).fit_transform(z)\n",
    "print(cross_val_score(logreg, Xscale, y, cv=3, scoring='roc_auc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the results here suggest that both doing PCA or increasing the number of components will improve model accuracy by roughly the same amount. That said, the reason it happens with 20 dimmensions vs 5 is likely due to the fact that more dimmensions suggests that the data was not compressed to smaller dimmensionality, suggesting less loss. However, PCA appears to have done it better than GRP because PCA attempts to keep the highly correlated features and minimizing reconstruction error.\n"
     ]
    }
   ],
   "source": [
    "print(\"the results here suggest that both doing PCA or increasing the number of components will improve model accuracy by roughly the same amount. That said, the reason it happens with 20 dimmensions vs 5 is likely due to the fact that more dimmensions suggests that the data was not compressed to smaller dimmensionality, suggesting less loss. However, PCA appears to have done it better than GRP because PCA attempts to keep the highly correlated features and minimizing reconstruction error.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67842935 0.52639664 0.60736856]\n"
     ]
    }
   ],
   "source": [
    "Xscale = PCA(n_components=5).fit_transform(x)\n",
    "Xscale = StandardScaler().fit_transform(Xscale)\n",
    "print(cross_val_score(logreg, Xscale, y, cv=3, scoring='roc_auc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we can see here that the scaling after PCA hurts the results of the model when compared to scaling before PCA. This is likely because the scaling is done to unit variance, which is what PCA tries to retain. Without this, PCA likely losses accuracy\n"
     ]
    }
   ],
   "source": [
    "print(\"we can see here that the scaling after PCA hurts the results of the model when compared to scaling before PCA. This is likely because the scaling is done to unit variance, which is what PCA tries to retain. Without this, PCA likely losses accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
