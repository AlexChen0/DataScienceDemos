{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import datasets\n",
    "cal = datasets.fetch_california_housing(as_frame = True)\n",
    "cal = cal.frame\n",
    "#cal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5530311140279562\n",
      "0.6698771068248724\n"
     ]
    }
   ],
   "source": [
    "#we want all but MedHouseVal row, so we drop and use for Y axis\n",
    "#other code fairly trivial, will not comment much\n",
    "X = cal.drop(\"MedHouseVal\", axis = 1)\n",
    "Y = cal[[\"MedHouseVal\"]]\n",
    "\n",
    "#linear regression + cross validation here\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_validate, cross_val_score\n",
    "linreg = LinearRegression()\n",
    "print(np.mean(sk.model_selection.cross_val_score(linreg, X, Y, cv=5)))\n",
    "\n",
    "#GBTR + cross validation here\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "#53 is my favorite number, so what the hell, why not\n",
    "GBTR = GradientBoostingRegressor(random_state = 53)\n",
    "print(np.mean(cross_val_score(GBTR, X, Y, cv=5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7770922179811391\n",
      "1 0.805059720676563\n",
      "2 0.8039119353448712\n",
      "3 0.8323531010526962\n",
      "4 0.8227404258635967\n",
      "5 0.8360151780915578\n",
      "6 0.8425793614945352\n",
      "7 0.8389187596104635\n"
     ]
    }
   ],
   "source": [
    "# Split Train_test, from documentation\n",
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# First we definte the parameters we explore with\n",
    "para = [[100, 3, .1], #default\n",
    "            [100, 3, .2], #learning rate +\n",
    "            [200, 3, .1], #estimators +\n",
    "            [100, 6, .1], #depth +\n",
    "            [200, 3, .2], #learning rate +, estimators +\n",
    "            [100, 6, .2], #learning rate +, depth +\n",
    "            [200, 6, .1], #estimators +, depth +\n",
    "            [200, 6, .2]] #all 3 + \n",
    "\n",
    "#now we can do the GRB on each of the parameters\n",
    "for i in range(len(para)):\n",
    "    GBR = GradientBoostingRegressor(learning_rate = para[i][2], \n",
    "                                      n_estimators= para[i][0], \n",
    "                                      max_depth = para[i][1], \n",
    "                                      random_state = 53)\n",
    "    GBR.fit(Xtrain, Ytrain)\n",
    "    score = GBR.score(Xtest, Ytest)\n",
    "    print(i, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best combination appears to be estimators and depth increase, with a score of .8426\n",
      "Clearly, it seems that linear regression has the lowest success of the models, with gradient boosting tree regression being much more successful. for the GBR tuning, we can see that a high depth makes the most difference, with learning rate and estimator count making positive change from the default parameters but not nearly as much as depth when doubling the numbers\n"
     ]
    }
   ],
   "source": [
    "print(\"the best combination appears to be estimators and depth increase, with a score of .8426\")\n",
    "\n",
    "print(\"Clearly, it seems that linear regression has the lowest success of the models, with gradient boosting tree regression being much more successful. for the GBR tuning, we can see that a high depth makes the most difference, with learning rate and estimator count making positive change from the default parameters but not nearly as much as depth when doubling the numbers\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7843023255813952\n",
      "0.7884205426356589\n"
     ]
    }
   ],
   "source": [
    "# transform the target, hopefully can still use X and Y and replace values only\n",
    "Y = cal[[\"MedHouseVal\"]].apply(lambda x: x> 2).astype(bool)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "print(np.mean(cross_val_score(logreg, X, Y, cv = 5)))\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "GBC = GradientBoostingClassifier(random_state = 53)\n",
    "print(np.mean(cross_val_score(GBC, X, Y, cv = 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5707364341085271\n",
      "1 0.5707364341085271\n",
      "2 0.6228197674418605\n",
      "3 0.6300872093023255\n"
     ]
    }
   ],
   "source": [
    "para = [[1, \"scale\"], #default\n",
    "        [2, \"scale\"], #reg +\n",
    "        [1, \"auto\"], #scale -> auto\n",
    "        [2, \"auto\"],] #both changes\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size = 0.2, random_state = 53)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "for i in range(len(para)):\n",
    "    SVM = SVC(C = para[i][0], gamma = para[i][1])\n",
    "    SVM.fit(Xtrain, Ytrain)\n",
    "    score = SVM.score(Xtest, Ytest)\n",
    "    print(i, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best combination appears to be a high reg coefficient and auto setting, with score of .6301\n",
      "interestingly, the tuning on SVM appears to have very low score compared with both the log regression and the gradient boosting classifier. That said, the gradient boosting classifier is more successful than log reg, and if SVM is needed, use auto and a high reg coefficient.\n"
     ]
    }
   ],
   "source": [
    "print(\"the best combination appears to be a high reg coefficient and auto setting, with score of .6301\")\n",
    "\n",
    "print(\"interestingly, the tuning on SVM appears to have very low score compared with both the log regression and the gradient boosting classifier. That said, the gradient boosting classifier is more successful than log reg, and if SVM is needed, use auto and a high reg coefficient.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
